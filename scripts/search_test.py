# import sys
# import os
# import torch
# import open_clip
# from pymilvus import connections, Collection

# # --- C·∫§U H√åNH ---
# # Ph·∫£i kh·ªõp v·ªõi l√∫c insert
# COLLECTION_NAME = 'video_search_vit_b_32' 
# MODEL_NAME = 'ViT-B-32'
# PRETRAINED = 'laion2b_s34b_b79k'

# def main():
#     # 1. Nh·∫≠p c√¢u truy v·∫•n
#     query_text = input("üëâ Nh·∫≠p t·ª´ kh√≥a t√¨m ki·∫øm (Ti·∫øng Anh): ")
#     if not query_text:
#         print("Vui l√≤ng nh·∫≠p g√¨ ƒë√≥...")
#         return

#     # 2. Load Model ƒë·ªÉ m√£ h√≥a vƒÉn b·∫£n (Text Encoding)
#     print("‚è≥ ƒêang load model ƒë·ªÉ hi·ªÉu vƒÉn b·∫£n...")
#     device = "cuda" if torch.cuda.is_available() else "cpu"
    
#     # Load model (ch·ªâ c·∫ßn ph·∫ßn text encoder)
#     model, _, _ = open_clip.create_model_and_transforms(
#         MODEL_NAME, pretrained=PRETRAINED, device=device
#     )
#     tokenizer = open_clip.get_tokenizer(MODEL_NAME)

#     # 3. Chuy·ªÉn vƒÉn b·∫£n th√†nh Vector
#     print(f"üîÑ ƒêang chuy·ªÉn '{query_text}' th√†nh vector...")
#     with torch.no_grad():
#         text_tokens = tokenizer([query_text]).to(device)
#         text_features = model.encode_text(text_tokens)
        
#         # Chu·∫©n h√≥a vector (Quan tr·ªçng ƒë·ªÉ t√≠nh kho·∫£ng c√°ch ch√≠nh x√°c)
#         text_features /= text_features.norm(dim=-1, keepdim=True)
        
#         # Chuy·ªÉn sang list ƒë·ªÉ g·ª≠i cho Milvus
#         query_vector = text_features.cpu().numpy()[0].tolist()

#     # 4. K·∫øt n·ªëi Milvus v√† Search
#     print("üöÄ ƒêang t√¨m trong Milvus...")
#     connections.connect("default", host="127.0.0.1", port="19530")
    
#     collection = Collection(COLLECTION_NAME)
#     collection.load() # ƒê·∫£m b·∫£o data ƒë√£ ·ªü tr√™n RAM

#     # C·∫•u h√¨nh t√¨m ki·∫øm
#     search_params = {
#         "metric_type": "L2", 
#         "params": {"nprobe": 10} # T√¨m trong 10 c·ª•m (tƒÉng l√™n n·∫øu mu·ªën t√¨m k·ªπ h∆°n)
#     }

#     results = collection.search(
#         data=[query_vector], 
#         anns_field="embedding", 
#         param=search_params, 
#         limit=5, # L·∫•y Top 5 k·∫øt qu·∫£
#         output_fields=["video_id", "frame_id", "path"] # L·∫•y th√™m th√¥ng tin ƒë·ªÉ hi·ªÉn th·ªã
#     )

#     # 5. Hi·ªÉn th·ªã k·∫øt qu·∫£
#     print("\n" + "="*30)
#     print(f"K·∫æT QU·∫¢ CHO: '{query_text}'")
#     print("="*30)
    
#     for hits in results:
#         for i, hit in enumerate(hits):
#             # L·∫•y th√¥ng tin
#             vid = hit.entity.get("video_id")
#             fid = hit.entity.get("frame_id")
#             path = hit.entity.get("path")
#             dist = hit.distance
            
#             print(f"Top {i+1} | Dist: {dist:.4f}")
#             print(f"   üé¨ Video: {vid} - Frame: {fid}")
#             print(f"   üìÇ Path:  {path}")
#             print("-" * 20)

# if __name__ == "__main__":
#     main()



import sys
import os
import torch
import open_clip
from PIL import Image
import matplotlib.pyplot as plt # Th∆∞ vi·ªán v·∫Ω ·∫£nh
from pymilvus import connections, Collection

# --- C·∫§U H√åNH (Kh·ªõp v·ªõi l√∫c Insert) ---
COLLECTION_NAME = 'video_search_vit_b_32'
MODEL_NAME = 'ViT-B-32'
PRETRAINED = 'laion2b_s34b_b79k'

def show_images(results):
    """H√†m v·∫Ω Grid ·∫£nh k·∫øt qu·∫£"""
    top_k = len(results[0])
    
    # T·∫°o khung h√¨nh (Figure)
    fig = plt.figure(figsize=(15, 6))
    plt.suptitle(f"Top {top_k} Results", fontsize=16)

    for i, hit in enumerate(results[0]):
        # L·∫•y th√¥ng tin
        path = hit.entity.get("path")
        dist = hit.distance
        video_id = hit.entity.get("video_id")
        
        # X·ª≠ l√Ω ƒë∆∞·ªùng d·∫´n (N·∫øu l∆∞u t∆∞∆°ng ƒë·ªëi th√¨ c·∫ßn n·ªëi v·ªõi th∆∞ m·ª•c g·ªëc)
        if not os.path.exists(path):
            # Th·ª≠ fix ƒë∆∞·ªùng d·∫´n n·∫øu ch·∫°y t·ª´ th∆∞ m·ª•c kh√°c
            path = os.path.join(os.getcwd(), path)
        
        # T·∫°o √¥ con (Subplot)
        ax = fig.add_subplot(1, top_k, i + 1)
        
        try:
            img = Image.open(path).convert('RGB')
            ax.imshow(img)
            # ƒê·∫∑t ti√™u ƒë·ªÅ cho t·ª´ng ·∫£nh
            ax.set_title(f"Rank {i+1}\nDist: {dist:.3f}\n{video_id}", color='green', fontsize=10)
        except Exception as e:
            print(f"Kh√¥ng load ƒë∆∞·ª£c ·∫£nh: {path}")
            ax.text(0.5, 0.5, "Image Not Found", ha='center', va='center')
        
        # ·∫®n tr·ª•c t·ªça ƒë·ªô cho ƒë·∫πp
        ax.axis('off')

    # Hi·ªÉn th·ªã c·ª≠a s·ªï
    print("‚úÖ ƒêang m·ªü c·ª≠a s·ªï k·∫øt qu·∫£...")
    plt.tight_layout()
    plt.show()

def main():
    # 1. Nh·∫≠p t·ª´ kh√≥a
    query_text = input("üëâ Nh·∫≠p m√¥ t·∫£ (Ti·∫øng Anh): ")
    if not query_text: return

    # 2. Load Model Text Encoder
    print("‚è≥ Loading model...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model, _, _ = open_clip.create_model_and_transforms(MODEL_NAME, pretrained=PRETRAINED, device=device)
    tokenizer = open_clip.get_tokenizer(MODEL_NAME)

    # 3. Encode Text
    with torch.no_grad():
        text_features = model.encode_text(tokenizer([query_text]).to(device))
        text_features /= text_features.norm(dim=-1, keepdim=True)
        query_vector = text_features.cpu().numpy()[0].tolist()

    # 4. Search Milvus
    connections.connect("default", host="127.0.0.1", port="19530")
    collection = Collection(COLLECTION_NAME)
    collection.load()

    search_params = {"metric_type": "L2", "params": {"nprobe": 10}}
    
    # L·∫•y Top 5 k·∫øt qu·∫£
    results = collection.search(
        data=[query_vector], 
        anns_field="embedding", 
        param=search_params, 
        limit=15, 
        output_fields=["path", "video_id"]
    )

    # 5. Hi·ªÉn th·ªã ·∫£nh
    show_images(results)

if __name__ == "__main__":
    main()